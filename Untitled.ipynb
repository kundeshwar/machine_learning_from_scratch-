{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "618c8512",
   "metadata": {},
   "source": [
    "### 1. Function to generate a data matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48caa7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b1cef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data(a, b):\n",
    "    c = np.matrix(a)\n",
    "    e = c.reshape(b,b)\n",
    "    return e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb4d736",
   "metadata": {},
   "source": [
    "### 2. Function generate a dependent varible column t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405b8000",
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_varible_1(x, w, b, e):\n",
    "    y = np.dot(x, w) + b\n",
    "    t = y + e\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8bcbd0",
   "metadata": {},
   "source": [
    "### 3.Function to compute a linear regression estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f37f1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_reg(x, w, b):\n",
    "    y = np.dot(x, w) + b\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d0d083",
   "metadata": {},
   "source": [
    "### 4.Function to compute a mean square error between t and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100215d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSQ_error(y, t):\n",
    "    n = y.shape[0]\n",
    "    msq_e = 0\n",
    "    for i in range(n):\n",
    "        msq_e = msq_e + (t[i] - y[i])**2\n",
    "        \n",
    "    msq_e = msq_e/n\n",
    "    return msq_e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a020b8",
   "metadata": {},
   "source": [
    "### 5.Function to Estimate The weights of linear regression using pseudo-inverse by assuming L2 regularizationÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c8fe03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pseudo_inverse(x, t, lambda_):\n",
    "    w_1 = np.dot(np.dot(np.linalg.inv(np.dot(np.transpose(x), x)), np.transpose(x)), t)\n",
    "    w = w_1[:3]\n",
    "    b = w_1[3]\n",
    "    x_1 = x[:,:3]\n",
    "    y = np.dot(w, x_1) + b\n",
    "    n = y.shape[0]\n",
    "    msq_e = 0\n",
    "    for i in range(n):\n",
    "        msq_e = msq_e + (t[i] - y[i])**2 \n",
    "    msq_e = msq_e/n\n",
    "    msq_e_1 = 0\n",
    "    for i in range(n):\n",
    "        msq_e_1 = msq_e_1 + w[i]**2\n",
    "    msq_e_1 = msq_e_1*lambda_\n",
    "    MSQ = msq_e_1 + msq_e\n",
    "    return f\"This is value of w: {w}\",f\"This is value of b: {b}\",f\"this is value of y: {y}\" ,f\"This is MSQ error : {MSQ}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b94ad4",
   "metadata": {},
   "source": [
    "### 6. Function to compute gradient of MSE with respective to weights vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31929a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(x, t, w, b):\n",
    "    y = np.dot(x, w) + b\n",
    "    m, n = x.shape\n",
    "    dj_dw = np.zeros((n,))\n",
    "    dj_db = 2*np.sum(y-t)/m\n",
    "    for i in range(m):\n",
    "        err = y[i] - t[i] \n",
    "        for j in range(n):\n",
    "            dj_dw[j] = dj_dw[j] + err*x[i, j]\n",
    "    dj_dw = (2*dj_dw)/m   \n",
    "    return dj_db, dj_dw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e9b5fc",
   "metadata": {},
   "source": [
    "### 7.Function to compute L norm of weight vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a18b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_2(w):\n",
    "    l2 = np.linalg.norm(a)\n",
    "    w_1 = w/l2\n",
    "    return l2, w_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef504221",
   "metadata": {},
   "source": [
    "### 8.Function to compute gradient of L2 Norm vector w.r.t weight vectorÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a5166f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, y, w, b): \n",
    "    m,n = X.shape          \n",
    "    dj_dw = np.zeros((n,))\n",
    "    dj_db = 0.\n",
    "\n",
    "    for i in range(m):                             \n",
    "        err = (np.dot(X[i], w) + b) - y[i]   \n",
    "        for j in range(n):                         \n",
    "            dj_dw[j] = dj_dw[j] + err * X[i, j]    \n",
    "        dj_db = dj_db + err                        \n",
    "    dj_dw = dj_dw / m                                \n",
    "    dj_db = dj_db / m                                \n",
    "        \n",
    "    return dj_db, dj_dw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b416c63c",
   "metadata": {},
   "source": [
    "### 9. Function to compute L norm of weight vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00e8ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(w):\n",
    "    w_1 = w**2\n",
    "    w_distan = (np.sum(w_1))**0.5\n",
    "    w_norm = w/w_distan\n",
    "    return w_norm , w_distan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1916471a",
   "metadata": {},
   "source": [
    "### 10. Function to compute gradient of L2 Norm vector w.r.t weight vectorÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d27c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, y, w, b): \n",
    "    m,n = X.shape          \n",
    "    dj_dw = np.zeros((n,))\n",
    "    dj_db = 0.\n",
    "\n",
    "    for i in range(m):                             \n",
    "        err = (np.dot(X[i], w) + b) - y[i]   \n",
    "        for j in range(n):                         \n",
    "            dj_dw[j] = dj_dw[j] + err * X[i, j]    \n",
    "        dj_db = dj_db + err                        \n",
    "    dj_dw = dj_dw / m                                \n",
    "    dj_db = dj_db / m                                \n",
    "        \n",
    "    return dj_db, dj_dw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26115a3",
   "metadata": {},
   "source": [
    "### 11.Function to find single update of weight in linear regression by using Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed977fd7",
   "metadata": {},
   "source": [
    "#### Gradient descent summary\n",
    "#### you have developed a linear model that predicts ğ‘“ğ‘¤,ğ‘(ğ‘¥(ğ‘–)) : ğ‘“ğ‘¤,ğ‘(ğ‘¥(ğ‘–))=ğ‘¤ğ‘¥(ğ‘–)+ğ‘(1) In linear regression, you utilize input training data to fit the parameters ğ‘¤ , ğ‘ by minimizing a measure of the error between our predictions ğ‘“ğ‘¤,ğ‘(ğ‘¥(ğ‘–)) and the actual data ğ‘¦(ğ‘–) . The measure is called the ğ‘ğ‘œğ‘ ğ‘¡ , ğ½(ğ‘¤,ğ‘) . In training you measure the cost over all of our training samples ğ‘¥(ğ‘–),ğ‘¦(ğ‘–)\n",
    "\n",
    "#### ğ½(ğ‘¤,ğ‘)=12ğ‘šâˆ‘ğ‘–=0ğ‘šâˆ’1(ğ‘“ğ‘¤,ğ‘(ğ‘¥(ğ‘–))âˆ’ğ‘¦(ğ‘–))2(2) gradient descent was described as:\n",
    "\n",
    "#### repeatğ‘¤ğ‘} until convergence:{=ğ‘¤âˆ’ğ›¼âˆ‚ğ½(ğ‘¤,ğ‘)âˆ‚ğ‘¤=ğ‘âˆ’ğ›¼âˆ‚ğ½(ğ‘¤,ğ‘)âˆ‚ğ‘(3) where, parameters ğ‘¤ , ğ‘ are updated simultaneously. The gradient is defined as: âˆ‚ğ½(ğ‘¤,ğ‘)âˆ‚ğ‘¤âˆ‚ğ½(ğ‘¤,ğ‘)âˆ‚ğ‘=1ğ‘šâˆ‘ğ‘–=0ğ‘šâˆ’1(ğ‘“ğ‘¤,ğ‘(ğ‘¥(ğ‘–))âˆ’ğ‘¦(ğ‘–))ğ‘¥(ğ‘–)=1ğ‘šâˆ‘ğ‘–=0ğ‘šâˆ’1(ğ‘“ğ‘¤,ğ‘(ğ‘¥(ğ‘–))âˆ’ğ‘¦(ğ‘–))(4)(5) Here simultaniously means that you calculate the partial derivatives for all the parameters before updating any of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4263e9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MQE_error(x, y, w, b):\n",
    "    m = x.shape[0] \n",
    "    cost = 0\n",
    "    for i in range(m):\n",
    "        f_wb = w * x[i] + b\n",
    "        cost = cost + (f_wb - y[i])**2\n",
    "    total_cost =  cost/m\n",
    "    return total_cost\n",
    "\n",
    "def gradient(x, y, w, b): \n",
    "    m = x.shape[0]    \n",
    "    dj_dw = 0\n",
    "    dj_db = 0\n",
    "    \n",
    "    for i in range(m):  \n",
    "        f_wb = w * x[i] + b \n",
    "        dj_dw_i = (f_wb - y[i]) * x[i] \n",
    "        dj_db_i = f_wb - y[i] \n",
    "        dj_db += dj_db_i\n",
    "        dj_dw += dj_dw_i \n",
    "    dj_dw = 2*dj_dw / m \n",
    "    dj_db = 2*dj_db / m \n",
    "        \n",
    "    return dj_dw, dj_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb5088b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, w_in, b_in, alpha, MQE_error, gradient_function): \n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    p_history = []\n",
    "    b = b_in\n",
    "    w = w_in\n",
    "    # Calculate the gradient and update the parameters using gradient_function\n",
    "    dj_dw, dj_db = gradient_function(x, y, w , b)     \n",
    "\n",
    "    # Update Parameters using equation (3) above\n",
    "    b = b - alpha * dj_db                            \n",
    "    w = w - alpha * dj_dw\n",
    "    J_history.append(MQE_error(x, y, w , b))\n",
    "    \n",
    "    return w, b, J_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6b614a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1.0, 2.0])  \n",
    "y = np.array([300.0, 500.0])\n",
    "alpha = 1.0e-2\n",
    "w = 0\n",
    "b = 0\n",
    "w_final, b_final, J_hist = gradient_descent(x , y, w, b , alpha, MQE_error, gradient)\n",
    "print(f\"This is value of update w : {w_final}, this is value of update b : {b_final} , this is value of MSE : {J_hist}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d29b042",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 12.Function to find final updated value of weight in linear regression by using Gradient descentÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f30b922",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMQE_error(x, y, w, b):\n",
    "    m = x.shape[0] \n",
    "    cost = 0\n",
    "    for i in range(m):\n",
    "        f_wb = w * x[i] + b\n",
    "        cost = cost + (f_wb - y[i])**2\n",
    "    total_cost =  (cost/m)**2\n",
    "    return total_cost\n",
    "\n",
    "def compute_gradient(x, y, w, b): \n",
    "    m = x.shape[0]    \n",
    "    dj_dw = 0\n",
    "    dj_db = 0\n",
    "    \n",
    "    for i in range(m):  \n",
    "        f_wb = w * x[i] + b \n",
    "        dj_dw_i = (f_wb - y[i]) * x[i] \n",
    "        dj_db_i = f_wb - y[i] \n",
    "        dj_db += dj_db_i\n",
    "        dj_dw += dj_dw_i \n",
    "    dj_dw = 2*dj_dw / m \n",
    "    dj_db = 2*dj_db / m \n",
    "        \n",
    "    return dj_dw, dj_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a6b981",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, w_in, b_in, alpha, num_iters, RMQE_error, gradient_function): \n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    p_history = []\n",
    "    b = b_in\n",
    "    w = w_in\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        # Calculate the gradient and update the parameters using gradient_function\n",
    "        dj_dw, dj_db = gradient_function(x, y, w , b)     \n",
    "\n",
    "        # Update Parameters using equation (3) above\n",
    "        b = b - alpha * dj_db                            \n",
    "        w = w - alpha * dj_dw                            \n",
    "\n",
    "        # Save cost J at each iteration\n",
    "        if i<100000:      # prevent resource exhaustion \n",
    "            J_history.append(MQE_error(x, y, w , b))\n",
    "            p_history.append([w,b])\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters/10) == 0:\n",
    "            print(f\"Iteration {i:4}: RMSE {J_history[-1]:0.2e} \",\n",
    "                  f\"dj_dw: {dj_dw: 0.3e}, dj_db: {dj_db: 0.3e}  \",\n",
    "                  f\"w: {w: 0.3e}, b:{b: 0.5e}\")\n",
    " \n",
    "    return w, b, J_history, p_history #return w and J,w history for graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c215577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize parameters\n",
    "import math\n",
    "x = np.array([1.0, 2.0])  \n",
    "y = np.array([300.0, 500.0])\n",
    "w = 0\n",
    "b = 0\n",
    "# some gradient descent settings\n",
    "iterations = 10000\n",
    "tmp_alpha = 1.0e-2\n",
    "# run gradient descent\n",
    "w_final, b_final, J_hist, p_hist = gradient_descent(x ,y, w, b, tmp_alpha, \n",
    "                                                    iterations, RMQE_error, compute_gradient)\n",
    "print(f\"(w,b) found by gradient descent: ({w_final:8.4f},{b_final:8.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc87c3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
